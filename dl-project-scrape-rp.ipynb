{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da30253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "                                                 url  \\\n",
      "0  https://www.espncricinfo.com/series/indian-pre...   \n",
      "1  https://www.espncricinfo.com/series/indian-pre...   \n",
      "2  https://www.espncricinfo.com/series/indian-pre...   \n",
      "3  https://www.espncricinfo.com/series/indian-pre...   \n",
      "4  https://www.espncricinfo.com/series/indian-pre...   \n",
      "\n",
      "                                     title       date  \\\n",
      "0                 Lights up over a new era  17-Apr-08   \n",
      "1         McCullum's record 158 leads rout  18-Apr-08   \n",
      "2  Magnificent Hussey inspires Chennai win  19-Apr-08   \n",
      "3        Gambhir and Dhawan seal Delhi win  19-Apr-08   \n",
      "4   Hussey clinches nail-biter for Kolkata  20-Apr-08   \n",
      "\n",
      "                                             content Unnamed: 4 Unnamed: 5  \\\n",
      "0  Packing plenty of oomph: Cameron White's big-h...        NaN        NaN   \n",
      "1  Kolkata Knight Riders222 for 3 (McCullum 158*)...        NaN        NaN   \n",
      "2  Chennai Super Kings240 for 5 (Hussey 116*, Rai...        NaN        NaN   \n",
      "3  Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...        NaN        NaN   \n",
      "4  Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...        NaN        NaN   \n",
      "\n",
      "  Unnamed: 6  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "\n",
      "Preprocessed Data:\n",
      "                                                 url  \\\n",
      "0  https://www.espncricinfo.com/series/indian-pre...   \n",
      "1  https://www.espncricinfo.com/series/indian-pre...   \n",
      "2  https://www.espncricinfo.com/series/indian-pre...   \n",
      "3  https://www.espncricinfo.com/series/indian-pre...   \n",
      "4  https://www.espncricinfo.com/series/indian-pre...   \n",
      "\n",
      "                                     title       date  \\\n",
      "0                 Lights up over a new era 2008-04-17   \n",
      "1         McCullum's record 158 leads rout 2008-04-18   \n",
      "2  Magnificent Hussey inspires Chennai win 2008-04-19   \n",
      "3        Gambhir and Dhawan seal Delhi win 2008-04-19   \n",
      "4   Hussey clinches nail-biter for Kolkata 2008-04-20   \n",
      "\n",
      "                                             content Unnamed: 4 Unnamed: 5  \\\n",
      "0  Packing plenty of oomph: Cameron White's big-h...        NaN        NaN   \n",
      "1  Kolkata Knight Riders222 for 3 (McCullum 158*)...        NaN        NaN   \n",
      "2  Chennai Super Kings240 for 5 (Hussey 116*, Rai...        NaN        NaN   \n",
      "3  Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...        NaN        NaN   \n",
      "4  Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...        NaN        NaN   \n",
      "\n",
      "  Unnamed: 6  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/3szy81ds4mx0klg4kp2z0hrm0000gn/T/ipykernel_49197/2888778011.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Preprocess the CSV File\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Users/hemantg/Desktop/dl-scraped-final-rp.csv'  # Update this path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Original Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Preprocessing\n",
    "# Remove extra whitespace and newlines in text columns\n",
    "df['title'] = df['title'].str.strip().replace(r'\\s+', ' ', regex=True)\n",
    "df['content'] = df['content'].str.strip().replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Standardize the date format\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Display preprocessed data\n",
    "print(\"\\nPreprocessed Data:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7587d41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (Cleaned):\n",
      "                                                 url  \\\n",
      "0  https://www.espncricinfo.com/series/indian-pre...   \n",
      "1  https://www.espncricinfo.com/series/indian-pre...   \n",
      "2  https://www.espncricinfo.com/series/indian-pre...   \n",
      "3  https://www.espncricinfo.com/series/indian-pre...   \n",
      "4  https://www.espncricinfo.com/series/indian-pre...   \n",
      "\n",
      "                                     title       date  \\\n",
      "0                 Lights up over a new era  17-Apr-08   \n",
      "1         McCullum's record 158 leads rout  18-Apr-08   \n",
      "2  Magnificent Hussey inspires Chennai win  19-Apr-08   \n",
      "3        Gambhir and Dhawan seal Delhi win  19-Apr-08   \n",
      "4   Hussey clinches nail-biter for Kolkata  20-Apr-08   \n",
      "\n",
      "                                             content  \n",
      "0  Packing plenty of oomph: Cameron White's big-h...  \n",
      "1  Kolkata Knight Riders222 for 3 (McCullum 158*)...  \n",
      "2  Chennai Super Kings240 for 5 (Hussey 116*, Rai...  \n",
      "3  Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...  \n",
      "4  Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...  \n",
      "\n",
      "Preprocessed Data:\n",
      "                                                 url  \\\n",
      "0  https://www.espncricinfo.com/series/indian-pre...   \n",
      "1  https://www.espncricinfo.com/series/indian-pre...   \n",
      "2  https://www.espncricinfo.com/series/indian-pre...   \n",
      "3  https://www.espncricinfo.com/series/indian-pre...   \n",
      "4  https://www.espncricinfo.com/series/indian-pre...   \n",
      "\n",
      "                                     title       date  \\\n",
      "0                 Lights up over a new era 2008-04-17   \n",
      "1         McCullum's record 158 leads rout 2008-04-18   \n",
      "2  Magnificent Hussey inspires Chennai win 2008-04-19   \n",
      "3        Gambhir and Dhawan seal Delhi win 2008-04-19   \n",
      "4   Hussey clinches nail-biter for Kolkata 2008-04-20   \n",
      "\n",
      "                                             content  \n",
      "0  Packing plenty of oomph: Cameron White's big-h...  \n",
      "1  Kolkata Knight Riders222 for 3 (McCullum 158*)...  \n",
      "2  Chennai Super Kings240 for 5 (Hussey 116*, Rai...  \n",
      "3  Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...  \n",
      "4  Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/3szy81ds4mx0klg4kp2z0hrm0000gn/T/ipykernel_49197/148232914.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Preprocess the CSV File\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Users/hemantg/Desktop/dl-scraped-final-rp.csv'  # Update this path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnamed columns\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Original Data (Cleaned):\")\n",
    "print(df.head())\n",
    "\n",
    "# Preprocessing\n",
    "# Remove extra whitespace and newlines in text columns\n",
    "df['title'] = df['title'].str.strip().replace(r'\\s+', ' ', regex=True)\n",
    "df['content'] = df['content'].str.strip().replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Standardize the date format\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Display preprocessed data\n",
    "print(\"\\nPreprocessed Data:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af33365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (Cleaned):\n",
      "                                                 url  \\\n",
      "0  https://www.espncricinfo.com/series/indian-pre...   \n",
      "1  https://www.espncricinfo.com/series/indian-pre...   \n",
      "2  https://www.espncricinfo.com/series/indian-pre...   \n",
      "3  https://www.espncricinfo.com/series/indian-pre...   \n",
      "4  https://www.espncricinfo.com/series/indian-pre...   \n",
      "\n",
      "                                     title       date  \\\n",
      "0                 Lights up over a new era  17-Apr-08   \n",
      "1         McCullum's record 158 leads rout  18-Apr-08   \n",
      "2  Magnificent Hussey inspires Chennai win  19-Apr-08   \n",
      "3        Gambhir and Dhawan seal Delhi win  19-Apr-08   \n",
      "4   Hussey clinches nail-biter for Kolkata  20-Apr-08   \n",
      "\n",
      "                                             content  \n",
      "0  Packing plenty of oomph: Cameron White's big-h...  \n",
      "1  Kolkata Knight Riders222 for 3 (McCullum 158*)...  \n",
      "2  Chennai Super Kings240 for 5 (Hussey 116*, Rai...  \n",
      "3  Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...  \n",
      "4  Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...  \n",
      "\n",
      "Preprocessed Data:\n",
      "                                                 url  \\\n",
      "0  https://www.espncricinfo.com/series/indian-pre...   \n",
      "1  https://www.espncricinfo.com/series/indian-pre...   \n",
      "2  https://www.espncricinfo.com/series/indian-pre...   \n",
      "3  https://www.espncricinfo.com/series/indian-pre...   \n",
      "4  https://www.espncricinfo.com/series/indian-pre...   \n",
      "\n",
      "                                     title       date  \\\n",
      "0                 Lights up over a new era 2008-04-17   \n",
      "1         McCullum's record 158 leads rout 2008-04-18   \n",
      "2  Magnificent Hussey inspires Chennai win 2008-04-19   \n",
      "3        Gambhir and Dhawan seal Delhi win 2008-04-19   \n",
      "4   Hussey clinches nail-biter for Kolkata 2008-04-20   \n",
      "\n",
      "                                             content  \n",
      "0  Packing plenty of oomph: Cameron White's big-h...  \n",
      "1  Kolkata Knight Riders222 for 3 (McCullum 158*)...  \n",
      "2  Chennai Super Kings240 for 5 (Hussey 116*, Rai...  \n",
      "3  Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...  \n",
      "4  Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Preprocess the CSV File\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Users/hemantg/Desktop/dl-scraped-final-rp.csv'  # Update this path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnamed columns\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Original Data (Cleaned):\")\n",
    "print(df.head())\n",
    "\n",
    "# Preprocessing\n",
    "# Remove extra whitespace and newlines in text columns\n",
    "df['title'] = df['title'].str.strip().replace(r'\\s+', ' ', regex=True)\n",
    "df['content'] = df['content'].str.strip().replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Standardize the date format explicitly\n",
    "df['date'] = pd.to_datetime(df['date'], format='%d-%b-%y', errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Display preprocessed data\n",
    "print(\"\\nPreprocessed Data:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9e269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed article 0: Extracted Contextual Features\n",
      "Processed article 1: Extracted Contextual Features\n",
      "Processed article 2: Extracted Contextual Features\n",
      "Processed article 3: Extracted Contextual Features\n",
      "Processed article 4: Extracted Contextual Features\n",
      "Processed article 5: Extracted Contextual Features\n",
      "Processed article 6: Extracted Contextual Features\n",
      "Processed article 7: Extracted Contextual Features\n",
      "Processed article 8: Extracted Contextual Features\n",
      "Processed article 9: Extracted Contextual Features\n",
      "Processed article 10: Extracted Contextual Features\n",
      "Processed article 11: Extracted Contextual Features\n",
      "Processed article 12: Extracted Contextual Features\n",
      "Processed article 13: Extracted Contextual Features\n",
      "Processed article 14: Extracted Contextual Features\n",
      "Processed article 15: Extracted Contextual Features\n",
      "Processed article 16: Extracted Contextual Features\n",
      "Processed article 17: Extracted Contextual Features\n",
      "Processed article 18: Extracted Contextual Features\n",
      "Processed article 19: Extracted Contextual Features\n",
      "Processed article 20: Extracted Contextual Features\n",
      "Processed article 21: Extracted Contextual Features\n",
      "Processed article 22: Extracted Contextual Features\n",
      "Processed article 23: Extracted Contextual Features\n",
      "Processed article 24: Extracted Contextual Features\n",
      "Processed article 25: Extracted Contextual Features\n",
      "Processed article 26: Extracted Contextual Features\n",
      "Processed article 27: Extracted Contextual Features\n",
      "Processed article 28: Extracted Contextual Features\n",
      "Processed article 29: Extracted Contextual Features\n",
      "Processed article 30: Extracted Contextual Features\n",
      "Processed article 31: Extracted Contextual Features\n",
      "Processed article 32: Extracted Contextual Features\n",
      "Processed article 33: Extracted Contextual Features\n",
      "Processed article 34: Extracted Contextual Features\n",
      "Processed article 35: Extracted Contextual Features\n",
      "Processed article 36: Extracted Contextual Features\n",
      "Processed article 37: Extracted Contextual Features\n",
      "Processed article 38: Extracted Contextual Features\n",
      "Processed article 39: Extracted Contextual Features\n",
      "Processed article 40: Extracted Contextual Features\n",
      "Processed article 41: Extracted Contextual Features\n",
      "Processed article 42: Extracted Contextual Features\n",
      "Processed article 43: Extracted Contextual Features\n",
      "Processed article 44: Extracted Contextual Features\n",
      "Processed article 45: Extracted Contextual Features\n",
      "Processed article 46: Extracted Contextual Features\n",
      "Processed article 47: Extracted Contextual Features\n",
      "Processed article 48: Extracted Contextual Features\n",
      "Processed article 49: Extracted Contextual Features\n",
      "Processed article 50: Extracted Contextual Features\n",
      "Processed article 51: Extracted Contextual Features\n",
      "Processed article 52: Extracted Contextual Features\n",
      "Processed article 53: Extracted Contextual Features\n",
      "Processed article 54: Extracted Contextual Features\n",
      "Processed article 55: Extracted Contextual Features\n",
      "Processed article 56: Extracted Contextual Features\n",
      "Processed article 57: Extracted Contextual Features\n",
      "Processed article 58: Extracted Contextual Features\n",
      "Processed article 59: Extracted Contextual Features\n",
      "Processed article 60: Extracted Contextual Features\n",
      "Processed article 61: Extracted Contextual Features\n",
      "Processed article 62: Extracted Contextual Features\n",
      "Processed article 63: Extracted Contextual Features\n",
      "Processed article 64: Extracted Contextual Features\n",
      "Processed article 65: Extracted Contextual Features\n",
      "Processed article 66: Extracted Contextual Features\n",
      "Processed article 67: Extracted Contextual Features\n",
      "Processed article 68: Extracted Contextual Features\n",
      "Processed article 69: Extracted Contextual Features\n",
      "Processed article 70: Extracted Contextual Features\n",
      "Processed article 71: Extracted Contextual Features\n",
      "Processed article 72: Extracted Contextual Features\n",
      "Processed article 73: Extracted Contextual Features\n",
      "Processed article 74: Extracted Contextual Features\n",
      "Processed article 75: Extracted Contextual Features\n",
      "Processed article 76: Extracted Contextual Features\n",
      "Processed article 77: Extracted Contextual Features\n",
      "Processed article 78: Extracted Contextual Features\n",
      "Processed article 79: Extracted Contextual Features\n",
      "Processed article 80: Extracted Contextual Features\n",
      "Processed article 81: Extracted Contextual Features\n",
      "Processed article 82: Extracted Contextual Features\n",
      "Processed article 83: Extracted Contextual Features\n",
      "Processed article 84: Extracted Contextual Features\n",
      "Processed article 85: Extracted Contextual Features\n",
      "Processed article 86: Extracted Contextual Features\n",
      "Processed article 87: Extracted Contextual Features\n",
      "Processed article 88: Extracted Contextual Features\n",
      "Processed article 89: Extracted Contextual Features\n",
      "Processed article 90: Extracted Contextual Features\n",
      "Processed article 91: Extracted Contextual Features\n",
      "Processed article 92: Extracted Contextual Features\n",
      "Processed article 93: Extracted Contextual Features\n",
      "Processed article 94: Extracted Contextual Features\n",
      "Processed article 95: Extracted Contextual Features\n",
      "Processed article 96: Extracted Contextual Features\n",
      "Processed article 97: Extracted Contextual Features\n",
      "Processed article 98: Extracted Contextual Features\n",
      "Processed article 99: Extracted Contextual Features\n",
      "Processed article 100: Extracted Contextual Features\n",
      "Processed article 101: Extracted Contextual Features\n",
      "Processed article 102: Extracted Contextual Features\n",
      "Processed article 103: Extracted Contextual Features\n",
      "Processed article 104: Extracted Contextual Features\n",
      "Processed article 105: Extracted Contextual Features\n",
      "Processed article 106: Extracted Contextual Features\n",
      "Processed article 107: Extracted Contextual Features\n",
      "Processed article 108: Extracted Contextual Features\n",
      "Processed article 109: Extracted Contextual Features\n",
      "Processed article 110: Extracted Contextual Features\n",
      "Processed article 111: Extracted Contextual Features\n",
      "Processed article 112: Extracted Contextual Features\n",
      "Processed article 113: Extracted Contextual Features\n",
      "Processed article 114: Extracted Contextual Features\n",
      "Processed article 115: Extracted Contextual Features\n",
      "Processed article 116: Extracted Contextual Features\n",
      "Processed article 117: Extracted Contextual Features\n",
      "Processed article 118: Extracted Contextual Features\n",
      "Processed article 119: Extracted Contextual Features\n",
      "Processed article 120: Extracted Contextual Features\n",
      "Processed article 121: Extracted Contextual Features\n",
      "Processed article 122: Extracted Contextual Features\n",
      "Processed article 123: Extracted Contextual Features\n",
      "Processed article 124: Extracted Contextual Features\n",
      "Processed article 125: Extracted Contextual Features\n",
      "Processed article 126: Extracted Contextual Features\n",
      "Processed article 127: Extracted Contextual Features\n",
      "Processed article 128: Extracted Contextual Features\n",
      "Processed article 129: Extracted Contextual Features\n",
      "Processed article 130: Extracted Contextual Features\n",
      "Processed article 131: Extracted Contextual Features\n",
      "Processed article 132: Extracted Contextual Features\n",
      "Processed article 133: Extracted Contextual Features\n",
      "Processed article 134: Extracted Contextual Features\n",
      "Processed article 135: Extracted Contextual Features\n",
      "Processed article 136: Extracted Contextual Features\n",
      "Processed article 137: Extracted Contextual Features\n",
      "Processed article 138: Extracted Contextual Features\n",
      "Processed article 139: Extracted Contextual Features\n",
      "Processed article 140: Extracted Contextual Features\n",
      "Processed article 141: Extracted Contextual Features\n",
      "Processed article 142: Extracted Contextual Features\n",
      "Processed article 143: Extracted Contextual Features\n",
      "Processed article 144: Extracted Contextual Features\n",
      "Processed article 145: Extracted Contextual Features\n",
      "Processed article 146: Extracted Contextual Features\n",
      "Processed article 147: Extracted Contextual Features\n",
      "Processed article 148: Extracted Contextual Features\n",
      "Processed article 149: Extracted Contextual Features\n",
      "Processed article 150: Extracted Contextual Features\n",
      "Processed article 151: Extracted Contextual Features\n",
      "Processed article 152: Extracted Contextual Features\n",
      "Processed article 153: Extracted Contextual Features\n",
      "Processed article 154: Extracted Contextual Features\n",
      "Processed article 155: Extracted Contextual Features\n",
      "Processed article 156: Extracted Contextual Features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed article 157: Extracted Contextual Features\n",
      "Processed article 158: Extracted Contextual Features\n",
      "Processed article 159: Extracted Contextual Features\n",
      "Processed article 160: Extracted Contextual Features\n",
      "Processed article 161: Extracted Contextual Features\n",
      "Processed article 162: Extracted Contextual Features\n",
      "Processed article 163: Extracted Contextual Features\n",
      "Processed article 164: Extracted Contextual Features\n",
      "Processed article 165: Extracted Contextual Features\n",
      "Processed article 166: Extracted Contextual Features\n",
      "Processed article 167: Extracted Contextual Features\n",
      "Processed article 168: Extracted Contextual Features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m batch_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Extract Contextual Features from Each Article\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: extract_dynamic_context_grok(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m], row\u001b[38;5;241m.\u001b[39mname), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Ensure Sorting by Date\u001b[39;00m\n\u001b[1;32m     80\u001b[0m batch_df \u001b[38;5;241m=\u001b[39m batch_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(v)\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 76\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     72\u001b[0m batch_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Extract Contextual Features from Each Article\u001b[39;00m\n\u001b[1;32m     75\u001b[0m batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: extract_dynamic_context_grok(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m], row\u001b[38;5;241m.\u001b[39mname), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Ensure Sorting by Date\u001b[39;00m\n\u001b[1;32m     80\u001b[0m batch_df \u001b[38;5;241m=\u001b[39m batch_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mextract_dynamic_context_grok\u001b[0;34m(text, row_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[0;32m---> 51\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(API_URL, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     54\u001b[0m         result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1056\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1057\u001b[0m         (\n\u001b[1;32m   1058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1064\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_default_certs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    420\u001b[0m     sock\u001b[38;5;241m=\u001b[39mconn,\n\u001b[1;32m    421\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[1;32m    422\u001b[0m     certfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[1;32m    423\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[1;32m    424\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[1;32m    425\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[1;32m    426\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[1;32m    427\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    428\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    429\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    436\u001b[0m     default_ssl_context\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mversion() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    440\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    437\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[1;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[1;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[1;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[1;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[1;32m    525\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1073\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1075\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the Grok API Key and Endpoint\n",
    "API_KEY = \"xai-t4eGlJ4RWrlHmUijC12dyDaVzQAbV4NBoJuWnXLWVBAt5gOD0zMmQX76CkF5f4ai9Qnp1sR9mVXZ9GDi\"\n",
    "API_URL = \"https://api.x.ai/v1/chat/completions\"\n",
    "\n",
    "# Function to Extract Contextual and Context-Discovered Features Using Grok\n",
    "def extract_dynamic_context_grok(text, row_index):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # New Prompt for Feature Discovery\n",
    "    prompt = f\"\"\"\n",
    "    You are a cricket performance analyst. Extract features from this cricket article related to predicting a player's future performance.\n",
    "\n",
    "    1. Extract the following features:\n",
    "       - Player Mentioned\n",
    "       - Runs Scored (if any)\n",
    "       - Wickets Taken (if any)\n",
    "       - Match Context (location, opponent)\n",
    "       - Mention of Injuries or Selection Updates\n",
    "       - Predicted Player Confidence Level (0 to 1)\n",
    "       - Reason for Praise or Criticism (short text)\n",
    "\n",
    "    2. Additionally, look for **context-specific features** related to predicting the player's future performance:\n",
    "       - Emerging performance indicators (e.g., tactical mentions, team dynamics).\n",
    "       - Strategy hints (e.g., promotion in batting order, bowling changes).\n",
    "       - Important insights **not listed above**.\n",
    "\n",
    "    3. Provide a **short justification** explaining why these features were selected.\n",
    "\n",
    "    Article: {text}\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"grok-beta\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a cricket performance analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            completion_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(f\"Processed article {row_index}: Extracted Contextual Features\")\n",
    "            return completion_text\n",
    "\n",
    "        elif response.status_code in [429, 500]:  # Retry on rate-limit or server error\n",
    "            print(f\"Rate limit or server error at index {row_index}. Retrying in 30 seconds...\")\n",
    "            time.sleep(30)\n",
    "        else:\n",
    "            print(f\"API Error at index {row_index}: {response.status_code}, {response.text}\")\n",
    "            return \"Error in response\"\n",
    "\n",
    "    return \"Max retries exceeded\"\n",
    "\n",
    "# Batch Processing with Date Sorting\n",
    "batch_size = 500\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[i:i+batch_size].copy()\n",
    "\n",
    "    # Extract Contextual Features from Each Article\n",
    "    batch_df['dynamic_features'] = batch_df.apply(\n",
    "        lambda row: extract_dynamic_context_grok(row['content'], row.name), axis=1\n",
    "    )\n",
    "\n",
    "    # Ensure Sorting by Date\n",
    "    batch_df = batch_df.sort_values(by='date', ascending=True)\n",
    "\n",
    "    # Save Progress After Each Batch\n",
    "    batch_df.to_csv(f'/Users/hemantg/Desktop/sentiment_analysis_batch_rp_{i}.csv', index=False)\n",
    "    print(f\"Saved batch {i} with dynamic context features to CSV (sorted by date).\")\n",
    "\n",
    "print(\"\\nDynamic Contextual Feature Extraction and Date Sorting Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3bac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/6b/f0/c25aab1845afc3583a808d0a9b844bb3842245d87b9791e588e6aba8ce3a/openai-1.57.1-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.57.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/c9/87/28f93b5373cbca74ac3c6fd6e2025113f1a73164beb7cd966cdaed88cf70/jiter-0.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading jiter-0.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/62/51/72c18c55cf2f46ff4f91ebcc8f75aa30f7305f3d726be3f4ebffb4ae972b/pydantic-2.10.3-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in ./anaconda3/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Obtaining dependency information for typing-extensions<5,>=4.11 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in ./anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/87/f5/72347bc88306acb359581ac4d52f23c0ef445b57157adedb9aee0cd689d2/httpcore-1.0.7-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for pydantic-core==2.27.1 from https://files.pythonhosted.org/packages/1c/00/0804e84a78b7fdb394fff4c4f429815a10e5e0993e6ae0e0b27dd20379ee/pydantic_core-2.27.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pydantic_core-2.27.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading openai-1.57.1-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.0-cp311-cp311-macosx_11_0_arm64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.1-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: typing-extensions, jiter, httpcore, distro, annotated-types, pydantic-core, httpx, pydantic, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.0 openai-1.57.1 pydantic-2.10.3 pydantic-core-2.27.1 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82c479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fccc372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>Lights up over a new era</td>\n",
       "      <td>2008-04-17</td>\n",
       "      <td>Packing plenty of oomph: Cameron White's big-h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>McCullum's record 158 leads rout</td>\n",
       "      <td>2008-04-18</td>\n",
       "      <td>Kolkata Knight Riders222 for 3 (McCullum 158*)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>Magnificent Hussey inspires Chennai win</td>\n",
       "      <td>2008-04-19</td>\n",
       "      <td>Chennai Super Kings240 for 5 (Hussey 116*, Rai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>Gambhir and Dhawan seal Delhi win</td>\n",
       "      <td>2008-04-19</td>\n",
       "      <td>Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>Hussey clinches nail-biter for Kolkata</td>\n",
       "      <td>2008-04-20</td>\n",
       "      <td>Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.espncricinfo.com/series/indian-pre...   \n",
       "1  https://www.espncricinfo.com/series/indian-pre...   \n",
       "2  https://www.espncricinfo.com/series/indian-pre...   \n",
       "3  https://www.espncricinfo.com/series/indian-pre...   \n",
       "4  https://www.espncricinfo.com/series/indian-pre...   \n",
       "\n",
       "                                     title       date  \\\n",
       "0                 Lights up over a new era 2008-04-17   \n",
       "1         McCullum's record 158 leads rout 2008-04-18   \n",
       "2  Magnificent Hussey inspires Chennai win 2008-04-19   \n",
       "3        Gambhir and Dhawan seal Delhi win 2008-04-19   \n",
       "4   Hussey clinches nail-biter for Kolkata 2008-04-20   \n",
       "\n",
       "                                             content Unnamed: 4  Unnamed: 5  \\\n",
       "0  Packing plenty of oomph: Cameron White's big-h...        NaN         NaN   \n",
       "1  Kolkata Knight Riders222 for 3 (McCullum 158*)...        NaN         NaN   \n",
       "2  Chennai Super Kings240 for 5 (Hussey 116*, Rai...        NaN         NaN   \n",
       "3  Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...        NaN         NaN   \n",
       "4  Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...        NaN         NaN   \n",
       "\n",
       "   Unnamed: 6  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (Excel file)\n",
    "df = pd.read_excel('/Users/hemantg/Desktop/dl-try1-scraping.xlsx')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b2b0eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>Lights up over a new era</td>\n",
       "      <td>2008-04-17</td>\n",
       "      <td>Packing plenty of oomph: Cameron White's big-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>McCullum's record 158 leads rout</td>\n",
       "      <td>2008-04-18</td>\n",
       "      <td>Kolkata Knight Riders222 for 3 (McCullum 158*)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>Magnificent Hussey inspires Chennai win</td>\n",
       "      <td>2008-04-19</td>\n",
       "      <td>Chennai Super Kings240 for 5 (Hussey 116*, Rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>Gambhir and Dhawan seal Delhi win</td>\n",
       "      <td>2008-04-19</td>\n",
       "      <td>Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.espncricinfo.com/series/indian-pre...</td>\n",
       "      <td>Hussey clinches nail-biter for Kolkata</td>\n",
       "      <td>2008-04-20</td>\n",
       "      <td>Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.espncricinfo.com/series/indian-pre...   \n",
       "1  https://www.espncricinfo.com/series/indian-pre...   \n",
       "2  https://www.espncricinfo.com/series/indian-pre...   \n",
       "3  https://www.espncricinfo.com/series/indian-pre...   \n",
       "4  https://www.espncricinfo.com/series/indian-pre...   \n",
       "\n",
       "                                     title       date  \\\n",
       "0                 Lights up over a new era 2008-04-17   \n",
       "1         McCullum's record 158 leads rout 2008-04-18   \n",
       "2  Magnificent Hussey inspires Chennai win 2008-04-19   \n",
       "3        Gambhir and Dhawan seal Delhi win 2008-04-19   \n",
       "4   Hussey clinches nail-biter for Kolkata 2008-04-20   \n",
       "\n",
       "                                             content  \n",
       "0  Packing plenty of oomph: Cameron White's big-h...  \n",
       "1  Kolkata Knight Riders222 for 3 (McCullum 158*)...  \n",
       "2  Chennai Super Kings240 for 5 (Hussey 116*, Rai...  \n",
       "3  Delhi Daredevils132 for 1 (Gambhir 58*, Dhawan...  \n",
       "4  Kolkata Knight Riders112 for 5 (Hussey 38*, Ex...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Extra Columns\n",
    "df_cleaned = df.drop(columns=['Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6'], errors='ignore')\n",
    "\n",
    "# Check Cleaned Data\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede95caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url        0\n",
      "title      0\n",
      "date       0\n",
      "content    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop Rows with Missing Content\n",
    "df_cleaned = df_cleaned.dropna(subset=['content'])\n",
    "\n",
    "# Check for Remaining Nulls\n",
    "print(df_cleaned.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa9e80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date Column to Datetime Format\n",
    "df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Check for Invalid Dates\n",
    "invalid_dates = df_cleaned[df_cleaned['date'].isnull()]\n",
    "if not invalid_dates.empty:\n",
    "    print(\"Invalid dates found:\", invalid_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dbec97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data Shape: (99, 4)\n"
     ]
    }
   ],
   "source": [
    "# Verify Data After Cleaning\n",
    "print(f\"Cleaned Data Shape: {df_cleaned.shape}\")\n",
    "\n",
    "# Save Cleaned File\n",
    "df_cleaned.to_csv('cleaned_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846180bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Extract features from this cricket article to predict a player's future performance:\n",
    "\n",
    "1. Extract Key Features:\n",
    "   - Player Mentioned\n",
    "   - Runs Scored\n",
    "   - Wickets Taken\n",
    "   - Match Context (Location, Opponent)\n",
    "   - Mention of Injuries or Selection Updates\n",
    "   - Predicted Player Confidence Level (0 to 1)\n",
    "   - Reason for Praise or Criticism (Short Text)\n",
    "\n",
    "2. Discover additional context-specific features:\n",
    "   - Tactical Hints (Role Changes, Strategy Adjustments)\n",
    "   - Emerging Performance Indicators\n",
    "\n",
    "Provide a **short justification** explaining why these features are relevant.\n",
    "\n",
    "Article: {article_content}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1fa8b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created input file: /Users/hemantg/Desktop/batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the Cleaned Data\n",
    "df = pd.read_csv('/Users/hemantg/Desktop/dl-try1-scraping.csv')\n",
    "\n",
    "# Define the JSONL Output File\n",
    "jsonl_file = '/Users/hemantg/Desktop/batch_input.jsonl'\n",
    "\n",
    "# Create JSONL File\n",
    "with open(jsonl_file, 'w') as outfile:\n",
    "    for _, row in df.iterrows():\n",
    "        prompt = prompt_template.format(article_content=row['content'])\n",
    "        request_data = {\n",
    "            \"model\": \"gpt-4.0-mini\",  # Choose the model version\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a cricket performance analyst.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "        # Write Each JSON Object as a New Line\n",
    "        json.dump(request_data, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "print(f\"Created input file: {jsonl_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ef563d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m head \u001b[38;5;241m/\u001b[39mUsers\u001b[38;5;241m/\u001b[39mhemantg\u001b[38;5;241m/\u001b[39mDesktop\u001b[38;5;241m/\u001b[39mbatch_input\u001b[38;5;241m.\u001b[39mjsonl\n",
      "\u001b[0;31mNameError\u001b[0m: name 'head' is not defined"
     ]
    }
   ],
   "source": [
    "head /Users/hemantg/Desktop/batch_input.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define OpenAI Batch API Endpoint and Headers\n",
    "batch_endpoint = 'https://api.openai.com/v1/batches'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {openai.api_key}',  # Replace with your actual API key\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the Batch Request Payload\n",
    "batch_payload = {\n",
    "    'input': input_file,\n",
    "    'output': 'batch_output.jsonl',\n",
    "    'model': 'gpt-4.0-mini',\n",
    "    'temperature': 0\n",
    "}\n",
    "\n",
    "# Submit the Batch Request\n",
    "response = requests.post(batch_endpoint, headers=headers, json=batch_payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    batch_id = response.json()['id']\n",
    "    print(f\"Batch submitted successfully. Batch ID: {batch_id}\")\n",
    "else:\n",
    "    print(f\"Batch submission failed: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e551b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./anaconda3/lib/python3.11/site-packages (1.57.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./anaconda3/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./anaconda3/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./anaconda3/lib/python3.11/site-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./anaconda3/lib/python3.11/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in ./anaconda3/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./anaconda3/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in ./anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in ./anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b68b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully. File ID: file-RYNwdHpttBnsffaGfQkkTf\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Upload the Local JSONL File\n",
    "response = openai.File.create(\n",
    "    file=open(\"/Users/hemantg/Desktop/batch_input.jsonl\", \"rb\"),  # Path to your local file\n",
    "    purpose=\"fine-tune\"  # Use 'answers' or 'fine-tune' depending on your use case\n",
    "\n",
    ")\n",
    "\n",
    "# Extract File ID\n",
    "file_id = response['id']\n",
    "print(f\"File uploaded successfully. File ID: {file_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89fdcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch submission failed: 400 - {\n",
      "  \"error\": {\n",
      "    \"message\": \"Missing required parameter: 'endpoint'.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": \"endpoint\",\n",
      "    \"code\": \"missing_required_parameter\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Batch API Endpoint and Headers\n",
    "batch_endpoint = 'https://api.openai.com/v1/batches'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer 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',  # Replace with your actual API key\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the Batch Request Payload (Corrected)\n",
    "batch_payload = {\n",
    "    \"input_file_id\": \"file-RYNwdHpttBnsffaGfQkkTf\",  # Correct Parameter\n",
    "    \"model\": \"gpt-4.0-mini\",  # Choose the correct model version\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "# Submit the Batch Request\n",
    "response = requests.post(batch_endpoint, headers=headers, json=batch_payload)\n",
    "\n",
    "# Handle Response\n",
    "if response.status_code == 200:\n",
    "    batch_id = response.json()['id']\n",
    "    print(f\"Batch submitted successfully. Batch ID: {batch_id}\")\n",
    "else:\n",
    "    print(f\"Batch submission failed: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0142e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch submission failed: 400 - {\n",
      "  \"error\": {\n",
      "    \"message\": \"Missing required parameter: 'completion_window'.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": \"completion_window\",\n",
      "    \"code\": \"missing_required_parameter\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Batch API Endpoint and Headers\n",
    "batch_endpoint = 'https://api.openai.com/v1/batches'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer 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',  # Replace with your actual API key\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the Corrected Batch Request Payload\n",
    "batch_payload = {\n",
    "    \"input_file_id\": \"file-RYNwdHpttBnsffaGfQkkTf\",  # Correct File ID\n",
    "    \"model\": \"gpt-4.0-mini\",  # Model Version\n",
    "    \"endpoint\": \"chat/completions\",  # Correct Endpoint\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "# Submit the Batch Request\n",
    "response = requests.post(batch_endpoint, headers=headers, json=batch_payload)\n",
    "\n",
    "# Handle Response\n",
    "if response.status_code == 200:\n",
    "    batch_id = response.json()['id']\n",
    "    print(f\"Batch submitted successfully. Batch ID: {batch_id}\")\n",
    "else:\n",
    "    print(f\"Batch submission failed: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe3828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch submission failed: 400 - {\n",
      "  \"error\": {\n",
      "    \"message\": \"Unknown parameter: 'model'.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": \"model\",\n",
      "    \"code\": \"unknown_parameter\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Batch API Endpoint and Headers\n",
    "batch_endpoint = 'https://api.openai.com/v1/batches'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer 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',  # Replace with your actual API key\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the Corrected Batch Request Payload\n",
    "batch_payload = {\n",
    "    \"input_file_id\": \"file-RYNwdHpttBnsffaGfQkkTf\",  # Correct File ID\n",
    "    \"model\": \"gpt-4.0-mini\",  # Model Version\n",
    "    \"endpoint\": \"chat/completions\",  # Correct Endpoint\n",
    "    \"completion_window\": 3600,  # Set to 1 hour (3600 seconds)\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "# Submit the Batch Request\n",
    "response = requests.post(batch_endpoint, headers=headers, json=batch_payload)\n",
    "\n",
    "# Handle Response\n",
    "if response.status_code == 200:\n",
    "    batch_id = response.json()['id']\n",
    "    print(f\"Batch submitted successfully. Batch ID: {batch_id}\")\n",
    "else:\n",
    "    print(f\"Batch submission failed: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb9fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch submission failed: 400 - {\n",
      "  \"error\": {\n",
      "    \"message\": \"Unknown parameter: 'task'.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": \"task\",\n",
      "    \"code\": \"unknown_parameter\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Batch API Endpoint and Headers\n",
    "batch_endpoint = 'https://api.openai.com/v1/batches'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer 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',  # Replace with your actual API key\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the Corrected Batch Request Payload\n",
    "batch_payload = {\n",
    "    \"input_file_id\": \"file-RYNwdHpttBnsffaGfQkkTf\",  # Correct File ID\n",
    "    \"endpoint\": \"chat/completions\",  # Correct Endpoint\n",
    "    \"completion_window\": 3600,  # Set to 1 hour (3600 seconds)\n",
    "    \"task\": {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a cricket performance analyst.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Extract relevant features for predicting cricket player performance, such as runs, wickets, \"\n",
    "                    \"injuries, and performance indicators based on the following article.\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0,\n",
    "        \"model\": \"gpt-4.0-mini\"  # Move 'model' under 'task'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Submit the Batch Request\n",
    "response = requests.post(batch_endpoint, headers=headers, json=batch_payload)\n",
    "\n",
    "# Handle Response\n",
    "if response.status_code == 200:\n",
    "    batch_id = response.json()['id']\n",
    "    print(f\"Batch submitted successfully. Batch ID: {batch_id}\")\n",
    "else:\n",
    "    print(f\"Batch submission failed: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa9a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch submission failed: 400 - {\n",
      "  \"error\": {\n",
      "    \"message\": \"Unknown parameter: 'task_parameters'.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": \"task_parameters\",\n",
      "    \"code\": \"unknown_parameter\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Batch API Endpoint and Headers\n",
    "batch_endpoint = 'https://api.openai.com/v1/batches'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer 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',  # Replace with your actual API key\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the Batch Request Payload\n",
    "batch_payload = {\n",
    "    \"input_file_id\": \"file-RYNwdHpttBnsffaGfQkkTf\",  # Replace with your actual file ID\n",
    "    \"endpoint\": \"/v1/chat/completions\",  # Specify the correct endpoint\n",
    "    \"completion_window\": \"24h\",  # Set the processing window\n",
    "    \"task_parameters\": {\n",
    "        \"model\": \"gpt-4.0-mini\",  # Specify the model\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Submit the Batch Request\n",
    "response = requests.post(batch_endpoint, headers=headers, json=batch_payload)\n",
    "\n",
    "# Handle Response\n",
    "if response.status_code == 200:\n",
    "    batch_id = response.json()['id']\n",
    "    print(f\"Batch submitted successfully. Batch ID: {batch_id}\")\n",
    "else:\n",
    "    print(f\"Batch submission failed: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed32f059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch submission failed: 400 - {\n",
      "  \"error\": {\n",
      "    \"message\": \"Unknown parameter: 'model'.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": \"model\",\n",
      "    \"code\": \"unknown_parameter\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Batch API Endpoint and Headers\n",
    "batch_endpoint = 'https://api.openai.com/v1/batches'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer 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',  # Replace with your actual API key\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Corrected Batch Request Payload\n",
    "batch_payload = {\n",
    "    \"input_file_id\": \"file-RYNwdHpttBnsffaGfQkkTf\",  # Correct File ID\n",
    "    \"endpoint\": \"chat/completions\",  # Correct Endpoint\n",
    "    \"completion_window\": \"24h\",  # Correct Completion Window\n",
    "    \"model\": \"gpt-4.0-mini\",  # Model Specification\n",
    "    \"temperature\": 0  # Temperature Control\n",
    "}\n",
    "\n",
    "# Submit the Batch Request\n",
    "response = requests.post(batch_endpoint, headers=headers, json=batch_payload)\n",
    "\n",
    "# Handle Response\n",
    "if response.status_code == 200:\n",
    "    batch_id = response.json()['id']\n",
    "    print(f\"Batch submitted successfully. Batch ID: {batch_id}\")\n",
    "else:\n",
    "    print(f\"Batch submission failed: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf251c55",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Process the file in chunks\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m read_jsonl_in_chunks(file_path, chunk_size):\n\u001b[0;32m---> 41\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Handle the responses as needed\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m responses:\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36mprocess_chunk\u001b[0;34m(chunk)\u001b[0m\n\u001b[1;32m     21\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m chunk:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Customize the API call as per your requirements\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     25\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4.0-mini\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m---> 26\u001b[0m         prompt\u001b[38;5;241m=\u001b[39m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     27\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     28\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m responses\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prompt'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Function to read the JSONL file in chunks\n",
    "def read_jsonl_in_chunks(file_path, chunk_size):\n",
    "    with open(file_path, 'r') as file:\n",
    "        chunk = []\n",
    "        for line in file:\n",
    "            chunk.append(json.loads(line))\n",
    "            if len(chunk) == chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    responses = []\n",
    "    for item in chunk:\n",
    "        # Customize the API call as per your requirements\n",
    "        response = openai.Completion.create(\n",
    "            model='gpt-4.0-mini',\n",
    "            prompt=item['prompt'],\n",
    "            temperature=0,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = '/Users/hemantg/Desktop/batch_input.jsonl'\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 10  # Adjust based on your needs\n",
    "\n",
    "# Process the file in chunks\n",
    "for chunk in read_jsonl_in_chunks(file_path, chunk_size):\n",
    "    responses = process_chunk(chunk)\n",
    "    # Handle the responses as needed\n",
    "    for response in responses:\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9ff0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/3szy81ds4mx0klg4kp2z0hrm0000gn/T/ipykernel_5037/562281141.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv('/Users/hemantg/Downloads/ipl-2008_2022-player_scores.csv', parse_dates=['match_date'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Read the CSV data\n",
    "df = pd.read_csv('/Users/hemantg/Downloads/ipl-2008_2022-player_scores.csv', parse_dates=['match_date'])\n",
    "\n",
    "# Transform the date format\n",
    "df['match_date'] = df['match_date'].dt.strftime('%d-%b-%y')\n",
    "\n",
    "# Save the transformed data\n",
    "df.to_csv('/Users/hemantg/Downloads/ipl-2008_2022-player_scores-updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV\n",
    "csv_file = '/Users/hemantg/Downloads/player_performance_with_embeddings_final.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Dictionary to store replacements\n",
    "name_mapping = {}\n",
    "\n",
    "# Prompt user for each unique player name\n",
    "for name in df['player_name'].unique():\n",
    "    corrected_name = input(f\"Enter the correct name for '{name}': \")\n",
    "    name_mapping[name] = corrected_name\n",
    "\n",
    "# Replace names in the DataFrame\n",
    "df['player_name'] = df['player_name'].replace(name_mapping)\n",
    "\n",
    "# Save the updated CSV\n",
    "updated_csv_file = '/Users/hemantg/Downloads/player_performance_with_embeddings_final-with-correct-names.csv'  # Replace with your desired output file path\n",
    "df.to_csv(updated_csv_file, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved as '{updated_csv_file}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
